{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924b4a20-9705-4075-ae1b-5aaa205b44b7",
   "metadata": {},
   "source": [
    "### IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2290179f-4c8a-4cdc-9ec1-1e0a800a3010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim.models import Word2Vec,FastText\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe7bc733-5575-43ef-8f1e-5771c0447db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:/Users/DELL8/AppData/Roaming/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization (splitting text into words/sentences)\n",
    "nltk.data.path.append(\"C:/Users/DELL8/AppData/Roaming/nltk_data\")\n",
    "nltk.download('punkt', download_dir=\"C:/Users/DELL8/AppData/Roaming/nltk_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af66615f-2376-4ae3-893d-9123dba4d41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DELL8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\DELL8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\DELL8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\DELL8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')  # Removing common words (e.g., \"the\", \"is\", \"and\")\n",
    "nltk.download('averaged_perceptron_tagger')  # POS tagging (identifying nouns, verbs, adjectives)\n",
    "nltk.download('wordnet')  # WordNet database for synonyms and lemmatization\n",
    "nltk.download('maxent_ne_chunker')  # Named Entity Recognition (NER) to identify names/locations\n",
    "nltk.download('words')  # Dictionary of valid English words (used for spell checking, NER)\n",
    "nltk.download('omw-1.4')  # Open Multilingual WordNet (support for multiple languages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d52c5ad-2227-472a-8ea3-f73c6e9d7913",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4be0bf-15a2-4e06-8941-5110599c57fb",
   "metadata": {},
   "source": [
    "#### READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e056bfc0-9e4b-4ea4-abe2-63b7e6c2a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path\n",
    "file_path = \"D:\\wiki file\\enwiki-latest-abstract3.xml\"\n",
    "\n",
    "text_data = []\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for _ in range(10000):  \n",
    "        text_data.append(file.readline().strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3624f-f3b0-47f4-80a5-ea1ceccd9b29",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1178a34a-432d-4453-9621-272fd76d8557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "raw_text = \" \".join(text_data)  \n",
    "clean_text = re.sub(r\"<.*?>\", \" \", raw_text)  \n",
    "clean_text = re.sub(r\"https?://\\S+\", \" \", clean_text) \n",
    "clean_text = re.sub(r\"[^a-zA-Z\\s]\", \" \", clean_text)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdd37f5f-b456-4eee-a61b-709b6639a353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Text (First 500 chars):      Wikipedia  Diego Maradona stadium       Diego Maradona stadium can refer to       All article disambiguation pages        All disambiguation pages        Place name disambiguation pages        Short description is different from Wikidata             Wikipedia  White Stone       White Stone may refer to       See also             Wikipedia  Yes Tor         grid ref UK   SX            In popular culture        References        External links             Wikipedia  Watermelon Man  composition\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Raw Text (First 500 chars):\", clean_text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186c4eb-0094-4a8a-a790-771bb9bbb8fd",
   "metadata": {},
   "source": [
    "#### Stopword Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7008cf-2028-4f13-bdf6-798966cf6701",
   "metadata": {},
   "source": [
    "#### Definition \n",
    "Stopword removal eliminates common words (e.g., \"the,\" \"and,\" \"is\") that add little meaning to text analysis, improving model efficiency. This helps focus on informative words, enhancing the accuracy of NLP tasks like vectorization and classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c68353f1-0732-49d8-a667-c96ccd1dac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_texts = []\n",
    "stopword_removed_texts = []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for _ in range(10000):  \n",
    "        line = file.readline().strip()\n",
    "\n",
    "        clean_line = re.sub(r\"<.*?>\", \" \", line)\n",
    "        clean_line = re.sub(r\"https?://\\S+\", \" \", clean_line)\n",
    "        clean_line = re.sub(r\"[^a-zA-Z\\s]\", \" \", clean_line)\n",
    "\n",
    "        words = clean_line.lower().split()\n",
    "\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        # Stopword Removal\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        \n",
    "        original_texts.append(clean_line)\n",
    "        stopword_removed_texts.append(\" \".join(filtered_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe1da51a-c5f3-4031-bc57-e4e9076d5534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Text</th>\n",
       "      <th>Without Stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia  Diego Maradona stadium</td>\n",
       "      <td>wikipedia diego maradona stadium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Diego Maradona stadium can refer to</td>\n",
       "      <td>diego maradona stadium refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All article disambiguation pages</td>\n",
       "      <td>article disambiguation pages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All disambiguation pages</td>\n",
       "      <td>disambiguation pages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Place name disambiguation pages</td>\n",
       "      <td>place name disambiguation pages</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Original Text                 Without Stopwords\n",
       "0       Wikipedia  Diego Maradona stadium   wikipedia diego maradona stadium\n",
       "1    Diego Maradona stadium can refer to        diego maradona stadium refer\n",
       "2    All article disambiguation pages           article disambiguation pages\n",
       "3            All disambiguation pages                   disambiguation pages\n",
       "4     Place name disambiguation pages        place name disambiguation pages"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame\n",
    "df_stopwords = pd.DataFrame({\"Original Text\": original_texts, \"Without Stopwords\": stopword_removed_texts})\n",
    "df_stopwords.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2965449a-2800-4268-aa05-dca7f84aa75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>references</td>\n",
       "      <td>616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>external</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>links</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>also</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>see</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>history</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>career</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>place</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>birth</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word  Score\n",
       "0     wikipedia    732\n",
       "27   references    616\n",
       "28     external    404\n",
       "29        links    404\n",
       "18         also    237\n",
       "17          see    213\n",
       "240     history    150\n",
       "195      career    122\n",
       "8         place    115\n",
       "425       birth    107"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cleaned_text = \" \".join(stopword_removed_texts)\n",
    "word_counts = Counter(all_cleaned_text.split())\n",
    "df_word_freq = pd.DataFrame(word_counts.items(), columns=[\"Word\", \"Score\"]).sort_values(by=\"Score\", ascending=False)\n",
    "df_word_freq.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf9f80-3d43-4991-a85c-5e20ad7598f0",
   "metadata": {},
   "source": [
    "### Observation\n",
    "Stopword removal filters out common words like **\"the\", \"is\", \"in\"**, making text **more meaningful and concise**. For example,  \n",
    " **Before:** *\"The Diego Maradona stadium is used for international football matches.\"*  \n",
    " **After:** *\"diego maradona stadium used international football\"*  \n",
    "\n",
    "This process **removes noise**, keeps **important words**, and improves **text analysis for NLP and machine learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b9946f-d308-468e-b4de-10b4d715ac7f",
   "metadata": {},
   "source": [
    "####  WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5048092-aac7-4cd8-8c12-392fa1376408",
   "metadata": {},
   "source": [
    "#### **WordNet Definition:**  \n",
    "WordNet is a lexical database for the English language that groups words into sets of synonyms called synsets, providing short definitions and usage examples. It captures relationships between words, such as synonyms, antonyms, hypernyms, and hyponyms, making it useful for natural language processing (NLP) tasks like text analysis, word similarity, and language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d10675dd-e902-4536-88bd-2311cce399f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_texts = []\n",
    "wordnet_transformed = []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for _ in range(10000):  \n",
    "        line = file.readline().strip()\n",
    "\n",
    "        clean_line = re.sub(r\"<.*?>\", \" \", line)\n",
    "        clean_line = re.sub(r\"https?://\\S+\", \" \", clean_line)\n",
    "        clean_line = re.sub(r\"[^a-zA-Z\\s]\", \" \", clean_line)\n",
    "\n",
    "        words = clean_line.lower().split()\n",
    "\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        synonyms = []\n",
    "        for word in words:\n",
    "            syns = wordnet.synsets(word)\n",
    "            if syns:\n",
    "                synonyms.append(syns[0].lemmas()[0].name()) \n",
    "            else:\n",
    "                synonyms.append(word)\n",
    "\n",
    "\n",
    "        original_texts.append(clean_line)\n",
    "        wordnet_transformed.append(\" \".join(synonyms))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "719a29aa-6d91-45c5-afaa-52e556abb533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Text</th>\n",
       "      <th>WordNet Synonyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia  Diego Maradona stadium</td>\n",
       "      <td>wikipedia diego maradona stadium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Diego Maradona stadium can refer to</td>\n",
       "      <td>diego maradona stadium can mention to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All article disambiguation pages</td>\n",
       "      <td>all article disambiguation page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All disambiguation pages</td>\n",
       "      <td>all disambiguation page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Place name disambiguation pages</td>\n",
       "      <td>topographic_point name disambiguation page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Short description is different from Wikidata...</td>\n",
       "      <td>short description be different from wikidata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wikipedia  White Stone</td>\n",
       "      <td>wikipedia White rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>White Stone may refer to</td>\n",
       "      <td>White rock May mention to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>See also</td>\n",
       "      <td>see besides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wikipedia  Yes Tor</td>\n",
       "      <td>wikipedia yes tor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Original Text  \\\n",
       "0                 Wikipedia  Diego Maradona stadium    \n",
       "1              Diego Maradona stadium can refer to     \n",
       "2              All article disambiguation pages        \n",
       "3                      All disambiguation pages        \n",
       "4               Place name disambiguation pages        \n",
       "5    Short description is different from Wikidata...   \n",
       "6                            Wikipedia  White Stone    \n",
       "7                         White Stone may refer to     \n",
       "8                                      See also        \n",
       "9                                Wikipedia  Yes Tor    \n",
       "\n",
       "                               WordNet Synonyms  \n",
       "0              wikipedia diego maradona stadium  \n",
       "1         diego maradona stadium can mention to  \n",
       "2               all article disambiguation page  \n",
       "3                       all disambiguation page  \n",
       "4    topographic_point name disambiguation page  \n",
       "5  short description be different from wikidata  \n",
       "6                          wikipedia White rock  \n",
       "7                     White rock May mention to  \n",
       "8                                   see besides  \n",
       "9                             wikipedia yes tor  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wordnet = pd.DataFrame({\"Original Text\": original_texts, \"WordNet Synonyms\": wordnet_transformed})\n",
    "df_wordnet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb1ca75b-e1fb-40e4-9783-9af08afa332e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>the</td>\n",
       "      <td>811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mention</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>of</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>and</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>links</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>external</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>inch</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>angstrom</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>be</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  Score\n",
       "64         the    811\n",
       "0    wikipedia    732\n",
       "5      mention    667\n",
       "72          of    603\n",
       "105        and    433\n",
       "34       links    404\n",
       "33    external    404\n",
       "30        inch    373\n",
       "44    angstrom    321\n",
       "15          be    272"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_wordnet_text = \" \".join(wordnet_transformed)\n",
    "word_counts = Counter(all_wordnet_text.split())\n",
    "\n",
    "df_wordnet_freq = pd.DataFrame(word_counts.items(), columns=[\"Word\", \"Score\"]).sort_values(by=\"Score\", ascending=False)\n",
    "df_wordnet_freq.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05501387-d413-49c8-9e56-defd51cccebc",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "The WordNet-based synonym replacement process in the code refines text preprocessing by standardizing word representations. It first cleans raw text by removing XML tags, URLs, and non-alphabetic characters, ensuring only meaningful words are processed. Each word is then mapped to its most relevant synonym using WordNet, replacing it when a synonym exists while retaining the original word if no match is found. This transformation reduces vocabulary complexity, enhances semantic consistency, and improves the performance of NLP and machine learning models in tasks like sentiment analysis, text classification, and search optimization by ensuring similar words are treated uniformly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eaed38-9ad5-420c-9feb-8ca1f24f5ce6",
   "metadata": {},
   "source": [
    "#### lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb295ef-61c7-4b7a-b5c4-6bb9bd666aad",
   "metadata": {},
   "source": [
    "##### **Lemmatizer Definition:**  \n",
    "A **lemmatizer** is a tool in natural language processing (NLP) that reduces words to their base or dictionary form (lemma) by considering the word’s meaning and context. Unlike stemming, which simply removes suffixes, lemmatization ensures that the root word is a valid English word. It is commonly used in text preprocessing for machine learning and NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "956b11e0-136a-473f-b2bf-abd7156dd067",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "original_texts = []\n",
    "lemmatized_texts = []\n",
    "\n",
    "for line in text_data:\n",
    "    clean_line = re.sub(r\"<.*?>\", \" \", line)\n",
    "    clean_line = re.sub(r\"https?://\\S+\", \" \", clean_line)\n",
    "    clean_line = re.sub(r\"[^a-zA-Z\\s]\", \" \", clean_line)\n",
    "\n",
    "    words = clean_line.lower().split()\n",
    "\n",
    "    if not words:\n",
    "        continue\n",
    "\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    original_texts.append(clean_line)\n",
    "    lemmatized_texts.append(\" \".join(lemmatized_words))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51d599a9-6b26-44bf-a706-ed313f52f367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Text</th>\n",
       "      <th>Lemmatized Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia  Diego Maradona stadium</td>\n",
       "      <td>wikipedia diego maradona stadium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Diego Maradona stadium can refer to</td>\n",
       "      <td>diego maradona stadium can refer to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All article disambiguation pages</td>\n",
       "      <td>all article disambiguation page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All disambiguation pages</td>\n",
       "      <td>all disambiguation page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Place name disambiguation pages</td>\n",
       "      <td>place name disambiguation page</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Original Text  \\\n",
       "0       Wikipedia  Diego Maradona stadium    \n",
       "1    Diego Maradona stadium can refer to     \n",
       "2    All article disambiguation pages        \n",
       "3            All disambiguation pages        \n",
       "4     Place name disambiguation pages        \n",
       "\n",
       "                       Lemmatized Text  \n",
       "0     wikipedia diego maradona stadium  \n",
       "1  diego maradona stadium can refer to  \n",
       "2      all article disambiguation page  \n",
       "3              all disambiguation page  \n",
       "4       place name disambiguation page  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lemmatized = pd.DataFrame({\"Original Text\": original_texts, \"Lemmatized Text\": lemmatized_texts})\n",
    "df_lemmatized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "749134e3-9338-4ab3-8f0e-96e9b4d1eda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>the</td>\n",
       "      <td>811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>reference</td>\n",
       "      <td>618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>of</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>and</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>link</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>external</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>a</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>in</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>is</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  Score\n",
       "65         the    811\n",
       "0    wikipedia    732\n",
       "33   reference    618\n",
       "73          of    603\n",
       "106        and    433\n",
       "35        link    409\n",
       "34    external    404\n",
       "45           a    392\n",
       "30          in    372\n",
       "15          is    250"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lemmatized_text = \" \".join(lemmatized_texts)\n",
    "word_counts = Counter(all_lemmatized_text.split())\n",
    "\n",
    "df_lemmatized_freq = pd.DataFrame(word_counts.items(), columns=[\"Word\", \"Score\"]).sort_values(by=\"Score\", ascending=False)\n",
    "df_lemmatized_freq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1eca573-0c8e-4089-ba19-9169a2b7c382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Text (First 500 chars): wikipedia diego maradona stadium diego maradona stadium can refer to all article disambiguation page all disambiguation page place name disambiguation page short description is different from wikidata wikipedia white stone white stone may refer to see also wikipedia yes tor grid ref uk sx in popular culture reference external link wikipedia watermelon man composition length herbie hancock version mongo santamar a version chart performance herbie hancock version other version sample personnel ref\n"
     ]
    }
   ],
   "source": [
    "lemmatized_full_text = \" \".join(lemmatized_texts)\n",
    "print(\"Lemmatized Text (First 500 chars):\", lemmatized_full_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af13ae-5521-4ee9-83b9-0c2d746bc93e",
   "metadata": {},
   "source": [
    "#### observation\n",
    "Lemmatization significantly enhances text processing by converting words to their base or dictionary form while preserving their meanings. Unlike stemming, which often trims words without considering context, lemmatization ensures grammatical correctness. In the processed Wikipedia dataset, words like *running* become *run* and *better* becomes *good*, improving consistency for NLP tasks. The frequency distribution also changes, as different word variations merge into a single root form, reducing redundancy. This is crucial in machine learning applications like text classification and sentiment analysis, where standardized vocabulary enhances model accuracy and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b161c3f5-f32e-4590-a3e5-9520f1978558",
   "metadata": {},
   "source": [
    "#### count_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cdf2d5-64d4-48f1-b66c-0c28146839e0",
   "metadata": {},
   "source": [
    "##### **Count Vectorizer Definition:**  \n",
    "A **Count Vectorizer** is a technique in Natural Language Processing (NLP) that converts text data into a matrix of token counts. It represents each document as a vector, where each feature corresponds to a unique word, and the value represents the word's frequency in the document. This method is widely used in text classification, sentiment analysis, and other machine learning applications involving textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "206065b2-5c6b-4613-a004-d5ccfe995b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = []\n",
    "for line in text_data:\n",
    "    clean_line = re.sub(r\"<.*?>\", \" \", line)\n",
    "    clean_line = re.sub(r\"https?://\\S+\", \" \", clean_line)\n",
    "    clean_line = re.sub(r\"[^a-zA-Z\\s]\", \" \", clean_line)\n",
    "    cleaned_text.append(clean_line)\n",
    "\n",
    "final_clean_text = \" \".join(cleaned_text)\n",
    "count_vectorizer = CountVectorizer(max_features=10000)\n",
    "count_vectors = count_vectorizer.fit_transform([final_clean_text])\n",
    "\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "word_frequencies = count_vectors.toarray().flatten()\n",
    "vectorized_df = pd.DataFrame(count_vectors.toarray(), columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65e320c1-bde0-4137-ac40-81dc8eac8bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ab', 'abandoned', 'abandonment', 'abapeba', 'abba', 'abbreviated',\n",
       "       'abc', 'abd', 'abduction', 'aberdeen'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80a956ce-d1e7-4f72-959e-13c6d38eb880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abapeba</th>\n",
       "      <th>abba</th>\n",
       "      <th>abbreviated</th>\n",
       "      <th>abc</th>\n",
       "      <th>abd</th>\n",
       "      <th>abduction</th>\n",
       "      <th>aberdeen</th>\n",
       "      <th>...</th>\n",
       "      <th>zeus</th>\n",
       "      <th>ziffer</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zong</th>\n",
       "      <th>zophorame</th>\n",
       "      <th>zophoryctes</th>\n",
       "      <th>zoua</th>\n",
       "      <th>zubayrids</th>\n",
       "      <th>zuleta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 5818 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ab  abandoned  abandonment  abapeba  abba  abbreviated  abc  abd  \\\n",
       "0   1          1            1        1     2            2    1    7   \n",
       "\n",
       "   abduction  aberdeen  ...  zeus  ziffer  zip  zone  zong  zophorame  \\\n",
       "0          1         1  ...     2       2    1     1     1          1   \n",
       "\n",
       "   zophoryctes  zoua  zubayrids  zuleta  \n",
       "0            1     1          1       1  \n",
       "\n",
       "[1 rows x 5818 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6c07545-1c81-4347-b021-845c7da2a860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5200</th>\n",
       "      <td>the</td>\n",
       "      <td>811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5681</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4329</th>\n",
       "      <td>references</td>\n",
       "      <td>616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3641</th>\n",
       "      <td>of</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>and</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2973</th>\n",
       "      <td>links</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>external</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>in</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>is</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>also</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  score\n",
       "5200         the    811\n",
       "5681   wikipedia    732\n",
       "4329  references    616\n",
       "3641          of    603\n",
       "238          and    433\n",
       "2973       links    404\n",
       "1877    external    404\n",
       "2540          in    372\n",
       "2658          is    250\n",
       "182         also    237"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df = pd.DataFrame({\"Word\": feature_names, \"score\": word_frequencies})\n",
    "score_df = score_df.sort_values(by=\"score\", ascending=False)\n",
    "score_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2289ef62-a0c9-4c8a-bbf3-f9cf0a012655",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "The Count Vectorizer converts cleaned text into a numerical format by counting word occurrences. It extracts the **top 10,000 most frequent words**, creating a matrix where each column represents a word and its frequency. The most common words are selected based on occurrence, making it useful for **text classification, topic modeling, and sentiment analysis** in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d4dbc-34b2-4716-9c88-590c16b7e3f4",
   "metadata": {},
   "source": [
    "#### Tf-Idf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4655bc3-7e3c-4861-a982-abf87b69816f",
   "metadata": {},
   "source": [
    "**TF-IDF Vectorizer:** It transforms text data into numerical representation by computing Term Frequency-Inverse Document Frequency (TF-IDF) scores, highlighting important words while reducing the weight of common words, making it useful for text mining and information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "39e2958e-d040-41a8-94a9-f844ee066f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = []\n",
    "for line in text_data:\n",
    "    clean_line = re.sub(r\"<.*?>\", \" \", line) \n",
    "    clean_line = re.sub(r\"https?://\\S+\", \" \", clean_line)  \n",
    "    clean_line = re.sub(r\"[^a-zA-Z\\s]\", \" \", clean_line)  \n",
    "    cleaned_text.append(clean_line.lower())  \n",
    "\n",
    "final_clean_text = \" \".join(cleaned_text)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform([final_clean_text])\n",
    "\n",
    "feature_names1 = tfidf_vectorizer.get_feature_names_out()\n",
    "word_frequencies = tfidf_vectors.toarray().flatten()\n",
    "vectorized_df = pd.DataFrame(tfidf_vectors.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc410869-51e4-4b50-9f3b-51cb9d9feaff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ab', 'abandoned', 'abandonment', 'abapeba', 'abba', 'abbreviated',\n",
       "       'abc', 'abd', 'abduction', 'aberdeen'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4fd10754-1477-47dc-8a0a-21b1b65f1948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abapeba</th>\n",
       "      <th>abba</th>\n",
       "      <th>abbreviated</th>\n",
       "      <th>abc</th>\n",
       "      <th>abd</th>\n",
       "      <th>abduction</th>\n",
       "      <th>aberdeen</th>\n",
       "      <th>...</th>\n",
       "      <th>zeus</th>\n",
       "      <th>ziffer</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zong</th>\n",
       "      <th>zophorame</th>\n",
       "      <th>zophoryctes</th>\n",
       "      <th>zoua</th>\n",
       "      <th>zubayrids</th>\n",
       "      <th>zuleta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.003984</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 5818 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ab  abandoned  abandonment   abapeba      abba  abbreviated  \\\n",
       "0  0.000569   0.000569     0.000569  0.000569  0.001138     0.001138   \n",
       "\n",
       "        abc       abd  abduction  aberdeen  ...      zeus    ziffer       zip  \\\n",
       "0  0.000569  0.003984   0.000569  0.000569  ...  0.001138  0.001138  0.000569   \n",
       "\n",
       "       zone      zong  zophorame  zophoryctes      zoua  zubayrids    zuleta  \n",
       "0  0.000569  0.000569   0.000569     0.000569  0.000569   0.000569  0.000569  \n",
       "\n",
       "[1 rows x 5818 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e0233b87-b090-4626-b949-06d8e009854c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5200</th>\n",
       "      <td>the</td>\n",
       "      <td>811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5681</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4329</th>\n",
       "      <td>references</td>\n",
       "      <td>616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3641</th>\n",
       "      <td>of</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>and</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2973</th>\n",
       "      <td>links</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>external</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>in</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>is</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>also</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  score\n",
       "5200         the    811\n",
       "5681   wikipedia    732\n",
       "4329  references    616\n",
       "3641          of    603\n",
       "238          and    433\n",
       "2973       links    404\n",
       "1877    external    404\n",
       "2540          in    372\n",
       "2658          is    250\n",
       "182         also    237"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_df = pd.DataFrame({\"Word\": feature_names1, \"score\": word_frequencies})\n",
    "value_df= score_df.sort_values(by=\"score\", ascending=False)\n",
    "value_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b374ee-4677-4252-988c-b61264a5a160",
   "metadata": {},
   "source": [
    "#### Observation \n",
    "The TF-IDF vectorization process transforms text data into numerical representations by weighting word importance based on term frequency (TF) and inverse document frequency (IDF). Unlike Count Vectorization, which represents raw word counts as integers, TF-IDF normalizes these counts, resulting in decimal values that highlight informative words while down-weighting common ones. If many words have similar TF-IDF scores, it indicates uniform distribution across the dataset. To improve variation, multiple documents should be used instead of a single merged text, and `max_df` and `min_df` parameters can help filter overly common or rare terms, enhancing feature significance for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51411f0a-b5f1-4324-b35e-3c0692552740",
   "metadata": {},
   "source": [
    "#### Word2Vec CBOW Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc1ed9a-92df-41e3-a0f1-4d9478dfc7f5",
   "metadata": {},
   "source": [
    "##### **Define**:\n",
    "A neural network-based word embedding technique that predicts a target word from its surrounding context, generating dense vectors that capture semantic relationships in text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23d32db9-8868-4fb5-bb75-b010d4650e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = []\n",
    "for line in text_data:\n",
    "    clean_line = re.sub(r\"<.*?>\", \" \", line)  \n",
    "    clean_line = re.sub(r\"https?://\\S+\", \" \", clean_line)  \n",
    "    clean_line = re.sub(r\"[^a-zA-Z\\s]\", \" \", clean_line) \n",
    "    cleaned_text.append(clean_line.lower())  \n",
    "\n",
    "tokenized_sentences = [sentence.split() for sentence in cleaned_text if sentence]\n",
    "\n",
    "word2vec_cbow = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=2, workers=4, sg=0)\n",
    "\n",
    "word2vec_cbow.save(\"word2vec_cbow.model\")\n",
    "\n",
    "vocabulary_size = len(word2vec_cbow.wv)\n",
    "vocabulary_words = list(word2vec_cbow.wv.index_to_key)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07584f5a-395e-470a-89ea-f1c61b9f19da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2284"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2757d37-ce39-428e-a802-125d11d84a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'wikipedia',\n",
       " 'references',\n",
       " 'of',\n",
       " 'and',\n",
       " 'links',\n",
       " 'external',\n",
       " 'in',\n",
       " 'a',\n",
       " 'is']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd4789a8-2bce-4ef5-98f8-322110f0b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_to_check = \"wikipedia\"  # Choose a word from vocabulary\n",
    "if word_to_check in word2vec_cbow.wv:\n",
    "     similar_words = word2vec_cbow.wv.most_similar(word_to_check, topn=5)\n",
    "     similar_words\n",
    "else:\n",
    "     word_to_check\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e21f91ed-663f-4fb9-a992-96804ce5acb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 0.9993048906326294),\n",
       " ('the', 0.9992998838424683),\n",
       " ('and', 0.9992989897727966),\n",
       " ('a', 0.9992642402648926),\n",
       " ('in', 0.999263346195221)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1bc28bce-8727-4cfc-8984-6797089d8f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = {word: word2vec_cbow.wv[word] for word in word2vec_cbow.wv.index_to_key}\n",
    "df_word2vec = pd.DataFrame.from_dict(word_vectors, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e378480-1d07-4f47-b9bd-8e7da49033f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.385390</td>\n",
       "      <td>0.285547</td>\n",
       "      <td>0.266091</td>\n",
       "      <td>0.495479</td>\n",
       "      <td>-0.089024</td>\n",
       "      <td>-1.021536</td>\n",
       "      <td>0.430143</td>\n",
       "      <td>1.147418</td>\n",
       "      <td>-0.467211</td>\n",
       "      <td>-0.273426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188782</td>\n",
       "      <td>-0.107095</td>\n",
       "      <td>0.078021</td>\n",
       "      <td>0.025839</td>\n",
       "      <td>0.796384</td>\n",
       "      <td>0.397781</td>\n",
       "      <td>-0.081653</td>\n",
       "      <td>-0.330557</td>\n",
       "      <td>0.025315</td>\n",
       "      <td>0.156305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikipedia</th>\n",
       "      <td>-0.175684</td>\n",
       "      <td>0.127861</td>\n",
       "      <td>0.119228</td>\n",
       "      <td>0.215319</td>\n",
       "      <td>-0.027707</td>\n",
       "      <td>-0.448489</td>\n",
       "      <td>0.184707</td>\n",
       "      <td>0.501727</td>\n",
       "      <td>-0.203970</td>\n",
       "      <td>-0.126046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084776</td>\n",
       "      <td>-0.048570</td>\n",
       "      <td>0.033271</td>\n",
       "      <td>0.004130</td>\n",
       "      <td>0.339736</td>\n",
       "      <td>0.174038</td>\n",
       "      <td>-0.026366</td>\n",
       "      <td>-0.141830</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>0.068599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>references</th>\n",
       "      <td>-0.000344</td>\n",
       "      <td>0.003415</td>\n",
       "      <td>-0.006516</td>\n",
       "      <td>-0.000882</td>\n",
       "      <td>0.007603</td>\n",
       "      <td>0.006320</td>\n",
       "      <td>-0.003209</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>-0.008775</td>\n",
       "      <td>0.005956</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004288</td>\n",
       "      <td>0.005623</td>\n",
       "      <td>0.009276</td>\n",
       "      <td>-0.004102</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>0.005803</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.008236</td>\n",
       "      <td>-0.006840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>-0.300644</td>\n",
       "      <td>0.226226</td>\n",
       "      <td>0.199073</td>\n",
       "      <td>0.366939</td>\n",
       "      <td>-0.055515</td>\n",
       "      <td>-0.776590</td>\n",
       "      <td>0.325622</td>\n",
       "      <td>0.873580</td>\n",
       "      <td>-0.344989</td>\n",
       "      <td>-0.213739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135479</td>\n",
       "      <td>-0.082839</td>\n",
       "      <td>0.057388</td>\n",
       "      <td>0.016090</td>\n",
       "      <td>0.608980</td>\n",
       "      <td>0.297687</td>\n",
       "      <td>-0.048295</td>\n",
       "      <td>-0.251821</td>\n",
       "      <td>0.017238</td>\n",
       "      <td>0.110806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>-0.300356</td>\n",
       "      <td>0.219499</td>\n",
       "      <td>0.191082</td>\n",
       "      <td>0.370471</td>\n",
       "      <td>-0.056502</td>\n",
       "      <td>-0.772723</td>\n",
       "      <td>0.326256</td>\n",
       "      <td>0.875517</td>\n",
       "      <td>-0.358209</td>\n",
       "      <td>-0.200261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150876</td>\n",
       "      <td>-0.088963</td>\n",
       "      <td>0.062072</td>\n",
       "      <td>0.016601</td>\n",
       "      <td>0.606711</td>\n",
       "      <td>0.293165</td>\n",
       "      <td>-0.048528</td>\n",
       "      <td>-0.241582</td>\n",
       "      <td>0.017149</td>\n",
       "      <td>0.118267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1         2         3         4         5   \\\n",
       "the        -0.385390  0.285547  0.266091  0.495479 -0.089024 -1.021536   \n",
       "wikipedia  -0.175684  0.127861  0.119228  0.215319 -0.027707 -0.448489   \n",
       "references -0.000344  0.003415 -0.006516 -0.000882  0.007603  0.006320   \n",
       "of         -0.300644  0.226226  0.199073  0.366939 -0.055515 -0.776590   \n",
       "and        -0.300356  0.219499  0.191082  0.370471 -0.056502 -0.772723   \n",
       "\n",
       "                  6         7         8         9   ...        90        91  \\\n",
       "the         0.430143  1.147418 -0.467211 -0.273426  ...  0.188782 -0.107095   \n",
       "wikipedia   0.184707  0.501727 -0.203970 -0.126046  ...  0.084776 -0.048570   \n",
       "references -0.003209  0.003781 -0.008775  0.005956  ... -0.004288  0.005623   \n",
       "of          0.325622  0.873580 -0.344989 -0.213739  ...  0.135479 -0.082839   \n",
       "and         0.326256  0.875517 -0.358209 -0.200261  ...  0.150876 -0.088963   \n",
       "\n",
       "                  92        93        94        95        96        97  \\\n",
       "the         0.078021  0.025839  0.796384  0.397781 -0.081653 -0.330557   \n",
       "wikipedia   0.033271  0.004130  0.339736  0.174038 -0.026366 -0.141830   \n",
       "references  0.009276 -0.004102  0.008772  0.005767  0.005803  0.000201   \n",
       "of          0.057388  0.016090  0.608980  0.297687 -0.048295 -0.251821   \n",
       "and         0.062072  0.016601  0.606711  0.293165 -0.048528 -0.241582   \n",
       "\n",
       "                  98        99  \n",
       "the         0.025315  0.156305  \n",
       "wikipedia  -0.000153  0.068599  \n",
       "references  0.008236 -0.006840  \n",
       "of          0.017238  0.110806  \n",
       "and         0.017149  0.118267  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word2vec.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1973d7-4e63-4d39-ab1d-536ede317cec",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "The **Word2Vec CBOW model** efficiently learns word representations by predicting target words from surrounding context, mapping words to numerical vectors while preserving semantic meaning. When trained on Wikipedia data, it captures relationships between words, making it useful for **synonym detection, sentiment analysis, topic modeling, and recommendation systems**. Frequent words like \"the\" and \"of\" may have similar vectors due to shared contexts, revealing linguistic patterns that enhance **document similarity analysis and other NLP applications**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd63e33-b298-471e-8bbf-a9a27c6edf0f",
   "metadata": {},
   "source": [
    "#### FastText Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337abbec-4f4e-4aba-93b0-fc111900214b",
   "metadata": {},
   "source": [
    "**Define:** An extension of Word2Vec that represents words as subword n-grams, allowing it to handle rare and misspelled words better. It is useful for tasks like text classification, word embeddings, and handling out-of-vocabulary words in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "560e271e-7d56-4355-8c3b-ec6daad4679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = []\n",
    "for line in text_data:\n",
    "    clean_line = re.sub(r\"<.*?>\", \" \", line)  \n",
    "    clean_line = re.sub(r\"https?://\\S+\", \" \", clean_line)  \n",
    "    clean_line = re.sub(r\"[^a-zA-Z\\s]\", \" \", clean_line) \n",
    "    cleaned_text.append(clean_line.lower())  \n",
    "\n",
    "tokenized_sentences = [sentence.split() for sentence in cleaned_text if sentence]\n",
    "fasttext_model = FastText(sentences=tokenized_sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
    "fasttext_model.save(\"fasttext.model\")\n",
    "\n",
    "vocabulary_size = len(fasttext_model.wv)\n",
    "vocabulary_words = list(fasttext_model.wv.index_to_key)[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c8df853-ba9d-4778-9a62-199a36ce412d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2284"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c0fa9de-4e1e-4bbb-a7eb-11d42ccaff6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'wikipedia',\n",
       " 'references',\n",
       " 'of',\n",
       " 'and',\n",
       " 'links',\n",
       " 'external',\n",
       " 'in',\n",
       " 'a',\n",
       " 'is']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b425d9c-5a79-426b-b576-3876f1f73cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_check = \"wikipedia\"\n",
    "if word_to_check in fasttext_model.wv:\n",
    "     similar_words = fasttext_model.wv.most_similar(word_to_check, topn=5)\n",
    "     similar_words\n",
    "else:\n",
    "     word_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "863998d1-303b-4970-b10c-d6ec3909c23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('international', 0.9999849796295166),\n",
       " ('station', 0.9999843835830688),\n",
       " ('national', 0.9999843239784241),\n",
       " ('publication', 0.999984085559845),\n",
       " ('expedition', 0.9999829530715942)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e95b082-104e-480c-8626-298b8cf18f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.535637</td>\n",
       "      <td>1.147564</td>\n",
       "      <td>-0.591173</td>\n",
       "      <td>0.477393</td>\n",
       "      <td>0.052769</td>\n",
       "      <td>0.085065</td>\n",
       "      <td>0.990745</td>\n",
       "      <td>0.172422</td>\n",
       "      <td>0.720564</td>\n",
       "      <td>-1.208794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.950110</td>\n",
       "      <td>-0.023041</td>\n",
       "      <td>0.112896</td>\n",
       "      <td>0.405702</td>\n",
       "      <td>-0.778697</td>\n",
       "      <td>0.887925</td>\n",
       "      <td>-0.171397</td>\n",
       "      <td>-0.663371</td>\n",
       "      <td>0.209901</td>\n",
       "      <td>0.448828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikipedia</th>\n",
       "      <td>-0.211070</td>\n",
       "      <td>0.454074</td>\n",
       "      <td>-0.232049</td>\n",
       "      <td>0.187324</td>\n",
       "      <td>0.020590</td>\n",
       "      <td>0.033715</td>\n",
       "      <td>0.389426</td>\n",
       "      <td>0.066042</td>\n",
       "      <td>0.284584</td>\n",
       "      <td>-0.479298</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.377023</td>\n",
       "      <td>-0.006668</td>\n",
       "      <td>0.043672</td>\n",
       "      <td>0.160568</td>\n",
       "      <td>-0.307041</td>\n",
       "      <td>0.351139</td>\n",
       "      <td>-0.069604</td>\n",
       "      <td>-0.259559</td>\n",
       "      <td>0.082919</td>\n",
       "      <td>0.177189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>references</th>\n",
       "      <td>-0.134610</td>\n",
       "      <td>0.285926</td>\n",
       "      <td>-0.147685</td>\n",
       "      <td>0.120408</td>\n",
       "      <td>0.014089</td>\n",
       "      <td>0.022382</td>\n",
       "      <td>0.248421</td>\n",
       "      <td>0.045221</td>\n",
       "      <td>0.179110</td>\n",
       "      <td>-0.302662</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.239710</td>\n",
       "      <td>-0.003927</td>\n",
       "      <td>0.028201</td>\n",
       "      <td>0.102699</td>\n",
       "      <td>-0.196141</td>\n",
       "      <td>0.224706</td>\n",
       "      <td>-0.042293</td>\n",
       "      <td>-0.165619</td>\n",
       "      <td>0.052345</td>\n",
       "      <td>0.112619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>-0.377520</td>\n",
       "      <td>0.814007</td>\n",
       "      <td>-0.418979</td>\n",
       "      <td>0.333845</td>\n",
       "      <td>0.045157</td>\n",
       "      <td>0.061924</td>\n",
       "      <td>0.695783</td>\n",
       "      <td>0.123879</td>\n",
       "      <td>0.513430</td>\n",
       "      <td>-0.853870</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674944</td>\n",
       "      <td>-0.012907</td>\n",
       "      <td>0.081634</td>\n",
       "      <td>0.287607</td>\n",
       "      <td>-0.547476</td>\n",
       "      <td>0.629127</td>\n",
       "      <td>-0.122063</td>\n",
       "      <td>-0.469286</td>\n",
       "      <td>0.147936</td>\n",
       "      <td>0.313336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>-0.428046</td>\n",
       "      <td>0.911175</td>\n",
       "      <td>-0.464959</td>\n",
       "      <td>0.379916</td>\n",
       "      <td>0.042159</td>\n",
       "      <td>0.072153</td>\n",
       "      <td>0.782290</td>\n",
       "      <td>0.132726</td>\n",
       "      <td>0.571708</td>\n",
       "      <td>-0.951361</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.753387</td>\n",
       "      <td>-0.018835</td>\n",
       "      <td>0.092998</td>\n",
       "      <td>0.324872</td>\n",
       "      <td>-0.612457</td>\n",
       "      <td>0.706031</td>\n",
       "      <td>-0.137475</td>\n",
       "      <td>-0.519714</td>\n",
       "      <td>0.166600</td>\n",
       "      <td>0.356908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1         2         3         4         5   \\\n",
       "the        -0.535637  1.147564 -0.591173  0.477393  0.052769  0.085065   \n",
       "wikipedia  -0.211070  0.454074 -0.232049  0.187324  0.020590  0.033715   \n",
       "references -0.134610  0.285926 -0.147685  0.120408  0.014089  0.022382   \n",
       "of         -0.377520  0.814007 -0.418979  0.333845  0.045157  0.061924   \n",
       "and        -0.428046  0.911175 -0.464959  0.379916  0.042159  0.072153   \n",
       "\n",
       "                  6         7         8         9   ...        90        91  \\\n",
       "the         0.990745  0.172422  0.720564 -1.208794  ... -0.950110 -0.023041   \n",
       "wikipedia   0.389426  0.066042  0.284584 -0.479298  ... -0.377023 -0.006668   \n",
       "references  0.248421  0.045221  0.179110 -0.302662  ... -0.239710 -0.003927   \n",
       "of          0.695783  0.123879  0.513430 -0.853870  ... -0.674944 -0.012907   \n",
       "and         0.782290  0.132726  0.571708 -0.951361  ... -0.753387 -0.018835   \n",
       "\n",
       "                  92        93        94        95        96        97  \\\n",
       "the         0.112896  0.405702 -0.778697  0.887925 -0.171397 -0.663371   \n",
       "wikipedia   0.043672  0.160568 -0.307041  0.351139 -0.069604 -0.259559   \n",
       "references  0.028201  0.102699 -0.196141  0.224706 -0.042293 -0.165619   \n",
       "of          0.081634  0.287607 -0.547476  0.629127 -0.122063 -0.469286   \n",
       "and         0.092998  0.324872 -0.612457  0.706031 -0.137475 -0.519714   \n",
       "\n",
       "                  98        99  \n",
       "the         0.209901  0.448828  \n",
       "wikipedia   0.082919  0.177189  \n",
       "references  0.052345  0.112619  \n",
       "of          0.147936  0.313336  \n",
       "and         0.166600  0.356908  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = {word: fasttext_model.wv[word] for word in fasttext_model.wv.index_to_key}\n",
    "df_fasttext = pd.DataFrame.from_dict(word_vectors, orient='index')\n",
    "df_fasttext.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25282d4-101d-4010-9e3f-229bb8d6d2a0",
   "metadata": {},
   "source": [
    "#### observation\n",
    "FastText is trained on tokenized sentences from Wikipedia data, creating dense word embeddings with **100-dimensional vectors**. The model considers **subword units (character n-grams)**, allowing it to handle out-of-vocabulary (OOV) words better than Word2Vec. The trained model is saved as `\"fasttext.model\"`, and key outputs include the **vocabulary size**, **sample words**, and **most similar words** to `\"wikipedia\"`. Since FastText captures morphological similarities, it helps in NLP tasks like **text classification, information retrieval, and named entity recognition (NER)** by generating meaningful word representations even for unseen words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
