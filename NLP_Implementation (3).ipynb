{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924b4a20-9705-4075-ae1b-5aaa205b44b7",
   "metadata": {},
   "source": [
    "### IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2290179f-4c8a-4cdc-9ec1-1e0a800a3010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim.models import Word2Vec,FastText\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fe7bc733-5575-43ef-8f1e-5771c0447db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:/Users/DELL8/AppData/Roaming/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization (splitting text into words/sentences)\n",
    "nltk.data.path.append(\"C:/Users/DELL8/AppData/Roaming/nltk_data\")\n",
    "nltk.download('punkt', download_dir=\"C:/Users/DELL8/AppData/Roaming/nltk_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "af66615f-2376-4ae3-893d-9123dba4d41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DELL8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\DELL8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\DELL8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\DELL8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')  # Removing common words (e.g., \"the\", \"is\", \"and\")\n",
    "nltk.download('averaged_perceptron_tagger')  # POS tagging (identifying nouns, verbs, adjectives)\n",
    "nltk.download('wordnet')  # WordNet database for synonyms and lemmatization\n",
    "nltk.download('maxent_ne_chunker')  # Named Entity Recognition (NER) to identify names/locations\n",
    "nltk.download('words')  # Dictionary of valid English words (used for spell checking, NER)\n",
    "nltk.download('omw-1.4')  # Open Multilingual WordNet (support for multiple languages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3d52c5ad-2227-472a-8ea3-f73c6e9d7913",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4be0bf-15a2-4e06-8941-5110599c57fb",
   "metadata": {},
   "source": [
    "#### READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e056bfc0-9e4b-4ea4-abe2-63b7e6c2a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path\n",
    "file_path = \"D:\\wiki file\\enwiki-latest-abstract3.xml\"\n",
    "\n",
    "text_data = []\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for _ in range(10000):  \n",
    "        text_data.append(file.readline().strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3624f-f3b0-47f4-80a5-ea1ceccd9b29",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1178a34a-432d-4453-9621-272fd76d8557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "raw_text = \" \".join(text_data)  \n",
    "clean_text = re.sub(r\"<.*?>\", \" \", raw_text)  \n",
    "clean_text = re.sub(r\"https?://\\S+\", \" \", clean_text) \n",
    "clean_text = re.sub(r\"[^a-zA-Z\\s]\", \" \", clean_text)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bdd37f5f-b456-4eee-a61b-709b6639a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Raw Text (First 500 chars):\", clean_text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186c4eb-0094-4a8a-a790-771bb9bbb8fd",
   "metadata": {},
   "source": [
    "#### Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c178cf0b-2af1-4003-a532-93817416fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text.lower().split()  # Simple whitespace-based tokenization\n",
    "stop_words = set(stopwords.words('english'))  # Load stopwords\n",
    "filtered_words = [word for word in words if word.isalnum() and word not in stop_words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c68353f1-0732-49d8-a667-c96ccd1dac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_texts = []\n",
    "stopword_removed_texts = []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for _ in range(10000):  \n",
    "        line = file.readline().strip()\n",
    "\n",
    "        clean_line = re.sub(r\"<.*?>\", \" \", line)\n",
    "        clean_line = re.sub(r\"https?://\\S+\", \" \", clean_line)\n",
    "        clean_line = re.sub(r\"[^a-zA-Z\\s]\", \" \", clean_line)\n",
    "\n",
    "        words = clean_line.lower().split()\n",
    "\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        # Stopword Removal\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        \n",
    "        original_texts.append(clean_line)\n",
    "        stopword_removed_texts.append(\" \".join(filtered_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fe1da51a-c5f3-4031-bc57-e4e9076d5534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Text</th>\n",
       "      <th>Without Stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia  Diego Maradona stadium</td>\n",
       "      <td>wikipedia diego maradona stadium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Diego Maradona stadium can refer to</td>\n",
       "      <td>diego maradona stadium refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All article disambiguation pages</td>\n",
       "      <td>article disambiguation pages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All disambiguation pages</td>\n",
       "      <td>disambiguation pages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Place name disambiguation pages</td>\n",
       "      <td>place name disambiguation pages</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Original Text                 Without Stopwords\n",
       "0       Wikipedia  Diego Maradona stadium   wikipedia diego maradona stadium\n",
       "1    Diego Maradona stadium can refer to        diego maradona stadium refer\n",
       "2    All article disambiguation pages           article disambiguation pages\n",
       "3            All disambiguation pages                   disambiguation pages\n",
       "4     Place name disambiguation pages        place name disambiguation pages"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame\n",
    "df_stopwords = pd.DataFrame({\"Original Text\": original_texts, \"Without Stopwords\": stopword_removed_texts})\n",
    "df_stopwords.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2965449a-2800-4268-aa05-dca7f84aa75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>references</td>\n",
       "      <td>616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>external</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>links</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>also</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>see</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>history</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>career</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>place</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>birth</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word  Score\n",
       "0     wikipedia    732\n",
       "27   references    616\n",
       "28     external    404\n",
       "29        links    404\n",
       "18         also    237\n",
       "17          see    213\n",
       "240     history    150\n",
       "195      career    122\n",
       "8         place    115\n",
       "425       birth    107"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cleaned_text = \" \".join(stopword_removed_texts)\n",
    "word_counts = Counter(all_cleaned_text.split())\n",
    "df_word_freq = pd.DataFrame(word_counts.items(), columns=[\"Word\", \"Score\"]).sort_values(by=\"Score\", ascending=False)\n",
    "df_word_freq.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf9f80-3d43-4991-a85c-5e20ad7598f0",
   "metadata": {},
   "source": [
    "### Observation\n",
    "Stopword removal filters out common words like **\"the\", \"is\", \"in\"**, making text **more meaningful and concise**. For example,  \n",
    " **Before:** *\"The Diego Maradona stadium is used for international football matches.\"*  \n",
    " **After:** *\"diego maradona stadium used international football\"*  \n",
    "\n",
    "This process **removes noise**, keeps **important words**, and improves **text analysis for NLP and machine learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b9946f-d308-468e-b4de-10b4d715ac7f",
   "metadata": {},
   "source": [
    "####  WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d10675dd-e902-4536-88bd-2311cce399f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_texts = []\n",
    "wordnet_transformed = []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for _ in range(10000):  \n",
    "        line = file.readline().strip()\n",
    "\n",
    "        clean_line = re.sub(r\"<.*?>\", \" \", line)\n",
    "        clean_line = re.sub(r\"https?://\\S+\", \" \", clean_line)\n",
    "        clean_line = re.sub(r\"[^a-zA-Z\\s]\", \" \", clean_line)\n",
    "\n",
    "        words = clean_line.lower().split()\n",
    "\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        synonyms = []\n",
    "        for word in words:\n",
    "            syns = wordnet.synsets(word)\n",
    "            if syns:\n",
    "                synonyms.append(syns[0].lemmas()[0].name()) \n",
    "            else:\n",
    "                synonyms.append(word)\n",
    "\n",
    "\n",
    "        original_texts.append(clean_line)\n",
    "        wordnet_transformed.append(\" \".join(synonyms))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "719a29aa-6d91-45c5-afaa-52e556abb533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Text</th>\n",
       "      <th>WordNet Synonyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia  Diego Maradona stadium</td>\n",
       "      <td>wikipedia diego maradona stadium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Diego Maradona stadium can refer to</td>\n",
       "      <td>diego maradona stadium can mention to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All article disambiguation pages</td>\n",
       "      <td>all article disambiguation page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All disambiguation pages</td>\n",
       "      <td>all disambiguation page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Place name disambiguation pages</td>\n",
       "      <td>topographic_point name disambiguation page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Short description is different from Wikidata...</td>\n",
       "      <td>short description be different from wikidata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wikipedia  White Stone</td>\n",
       "      <td>wikipedia White rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>White Stone may refer to</td>\n",
       "      <td>White rock May mention to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>See also</td>\n",
       "      <td>see besides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wikipedia  Yes Tor</td>\n",
       "      <td>wikipedia yes tor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Original Text  \\\n",
       "0                 Wikipedia  Diego Maradona stadium    \n",
       "1              Diego Maradona stadium can refer to     \n",
       "2              All article disambiguation pages        \n",
       "3                      All disambiguation pages        \n",
       "4               Place name disambiguation pages        \n",
       "5    Short description is different from Wikidata...   \n",
       "6                            Wikipedia  White Stone    \n",
       "7                         White Stone may refer to     \n",
       "8                                      See also        \n",
       "9                                Wikipedia  Yes Tor    \n",
       "\n",
       "                               WordNet Synonyms  \n",
       "0              wikipedia diego maradona stadium  \n",
       "1         diego maradona stadium can mention to  \n",
       "2               all article disambiguation page  \n",
       "3                       all disambiguation page  \n",
       "4    topographic_point name disambiguation page  \n",
       "5  short description be different from wikidata  \n",
       "6                          wikipedia White rock  \n",
       "7                     White rock May mention to  \n",
       "8                                   see besides  \n",
       "9                             wikipedia yes tor  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wordnet = pd.DataFrame({\"Original Text\": original_texts, \"WordNet Synonyms\": wordnet_transformed})\n",
    "df_wordnet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "fb1ca75b-e1fb-40e4-9783-9af08afa332e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>the</td>\n",
       "      <td>811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mention</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>of</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>and</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>links</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>external</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>inch</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>angstrom</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>be</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  Score\n",
       "64         the    811\n",
       "0    wikipedia    732\n",
       "5      mention    667\n",
       "72          of    603\n",
       "105        and    433\n",
       "34       links    404\n",
       "33    external    404\n",
       "30        inch    373\n",
       "44    angstrom    321\n",
       "15          be    272"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_wordnet_text = \" \".join(wordnet_transformed)\n",
    "word_counts = Counter(all_wordnet_text.split())\n",
    "\n",
    "df_wordnet_freq = pd.DataFrame(word_counts.items(), columns=[\"Word\", \"Score\"]).sort_values(by=\"Score\", ascending=False)\n",
    "df_wordnet_freq.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05501387-d413-49c8-9e56-defd51cccebc",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "The WordNet-based synonym replacement process in the code refines text preprocessing by standardizing word representations. It first cleans raw text by removing XML tags, URLs, and non-alphabetic characters, ensuring only meaningful words are processed. Each word is then mapped to its most relevant synonym using WordNet, replacing it when a synonym exists while retaining the original word if no match is found. This transformation reduces vocabulary complexity, enhances semantic consistency, and improves the performance of NLP and machine learning models in tasks like sentiment analysis, text classification, and search optimization by ensuring similar words are treated uniformly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eaed38-9ad5-420c-9feb-8ca1f24f5ce6",
   "metadata": {},
   "source": [
    "#### lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "956b11e0-136a-473f-b2bf-abd7156dd067",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_texts = []\n",
    "lemmatized_texts = []\n",
    "\n",
    "for line in text_data:\n",
    "    clean_line = re.sub(r\"<.*?>\", \" \", line)\n",
    "    clean_line = re.sub(r\"https?://\\S+\", \" \", clean_line)\n",
    "    clean_line = re.sub(r\"[^a-zA-Z\\s]\", \" \", clean_line)\n",
    "\n",
    "    words = clean_line.lower().split()\n",
    "\n",
    "    if not words:\n",
    "        continue\n",
    "\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    original_texts.append(clean_line)\n",
    "    lemmatized_texts.append(\" \".join(lemmatized_words))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "51d599a9-6b26-44bf-a706-ed313f52f367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Text</th>\n",
       "      <th>Lemmatized Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia  Diego Maradona stadium</td>\n",
       "      <td>wikipedia diego maradona stadium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Diego Maradona stadium can refer to</td>\n",
       "      <td>diego maradona stadium can refer to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All article disambiguation pages</td>\n",
       "      <td>all article disambiguation page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All disambiguation pages</td>\n",
       "      <td>all disambiguation page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Place name disambiguation pages</td>\n",
       "      <td>place name disambiguation page</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Original Text  \\\n",
       "0       Wikipedia  Diego Maradona stadium    \n",
       "1    Diego Maradona stadium can refer to     \n",
       "2    All article disambiguation pages        \n",
       "3            All disambiguation pages        \n",
       "4     Place name disambiguation pages        \n",
       "\n",
       "                       Lemmatized Text  \n",
       "0     wikipedia diego maradona stadium  \n",
       "1  diego maradona stadium can refer to  \n",
       "2      all article disambiguation page  \n",
       "3              all disambiguation page  \n",
       "4       place name disambiguation page  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lemmatized = pd.DataFrame({\"Original Text\": original_texts, \"Lemmatized Text\": lemmatized_texts})\n",
    "df_lemmatized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "749134e3-9338-4ab3-8f0e-96e9b4d1eda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>the</td>\n",
       "      <td>811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>reference</td>\n",
       "      <td>618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>of</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>and</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>link</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>external</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>a</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>in</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>is</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  Score\n",
       "65         the    811\n",
       "0    wikipedia    732\n",
       "33   reference    618\n",
       "73          of    603\n",
       "106        and    433\n",
       "35        link    409\n",
       "34    external    404\n",
       "45           a    392\n",
       "30          in    372\n",
       "15          is    250"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lemmatized_text = \" \".join(lemmatized_texts)\n",
    "word_counts = Counter(all_lemmatized_text.split())\n",
    "\n",
    "df_lemmatized_freq = pd.DataFrame(word_counts.items(), columns=[\"Word\", \"Score\"]).sort_values(by=\"Score\", ascending=False)\n",
    "df_lemmatized_freq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f1eca573-0c8e-4089-ba19-9169a2b7c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_full_text = \" \".join(lemmatized_texts)\n",
    "print(\"Lemmatized Text (First 500 chars):\", lemmatized_full_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af13ae-5521-4ee9-83b9-0c2d746bc93e",
   "metadata": {},
   "source": [
    "#### observation\n",
    "Lemmatization significantly enhances text processing by converting words to their base or dictionary form while preserving their meanings. Unlike stemming, which often trims words without considering context, lemmatization ensures grammatical correctness. In the processed Wikipedia dataset, words like *running* become *run* and *better* becomes *good*, improving consistency for NLP tasks. The frequency distribution also changes, as different word variations merge into a single root form, reducing redundancy. This is crucial in machine learning applications like text classification and sentiment analysis, where standardized vocabulary enhances model accuracy and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b161c3f5-f32e-4590-a3e5-9520f1978558",
   "metadata": {},
   "source": [
    "#### count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "206065b2-5c6b-4613-a004-d5ccfe995b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = []\n",
    "for line in text_data:\n",
    "    clean_line = re.sub(r\"<.*?>\", \" \", line)\n",
    "    clean_line = re.sub(r\"https?://\\S+\", \" \", clean_line)\n",
    "    clean_line = re.sub(r\"[^a-zA-Z\\s]\", \" \", clean_line)\n",
    "    cleaned_text.append(clean_line)\n",
    "\n",
    "final_clean_text = \" \".join(cleaned_text)\n",
    "count_vectorizer = CountVectorizer(max_features=10000)\n",
    "count_vectors = count_vectorizer.fit_transform([final_clean_text])\n",
    "\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "word_frequencies = count_vectors.toarray().flatten()\n",
    "vectorized_df = pd.DataFrame(count_vectors.toarray(), columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "65e320c1-bde0-4137-ac40-81dc8eac8bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ab', 'abandoned', 'abandonment', 'abapeba', 'abba', 'abbreviated',\n",
       "       'abc', 'abd', 'abduction', 'aberdeen'], dtype=object)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "80a956ce-d1e7-4f72-959e-13c6d38eb880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abapeba</th>\n",
       "      <th>abba</th>\n",
       "      <th>abbreviated</th>\n",
       "      <th>abc</th>\n",
       "      <th>abd</th>\n",
       "      <th>abduction</th>\n",
       "      <th>aberdeen</th>\n",
       "      <th>...</th>\n",
       "      <th>zeus</th>\n",
       "      <th>ziffer</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zong</th>\n",
       "      <th>zophorame</th>\n",
       "      <th>zophoryctes</th>\n",
       "      <th>zoua</th>\n",
       "      <th>zubayrids</th>\n",
       "      <th>zuleta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 5818 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ab  abandoned  abandonment  abapeba  abba  abbreviated  abc  abd  \\\n",
       "0   1          1            1        1     2            2    1    7   \n",
       "\n",
       "   abduction  aberdeen  ...  zeus  ziffer  zip  zone  zong  zophorame  \\\n",
       "0          1         1  ...     2       2    1     1     1          1   \n",
       "\n",
       "   zophoryctes  zoua  zubayrids  zuleta  \n",
       "0            1     1          1       1  \n",
       "\n",
       "[1 rows x 5818 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d6c07545-1c81-4347-b021-845c7da2a860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5200</th>\n",
       "      <td>the</td>\n",
       "      <td>811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5681</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4329</th>\n",
       "      <td>references</td>\n",
       "      <td>616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3641</th>\n",
       "      <td>of</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>and</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2973</th>\n",
       "      <td>links</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>external</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>in</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>is</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>also</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  score\n",
       "5200         the    811\n",
       "5681   wikipedia    732\n",
       "4329  references    616\n",
       "3641          of    603\n",
       "238          and    433\n",
       "2973       links    404\n",
       "1877    external    404\n",
       "2540          in    372\n",
       "2658          is    250\n",
       "182         also    237"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df = pd.DataFrame({\"Word\": feature_names, \"score\": word_frequencies})\n",
    "score_df = score_df.sort_values(by=\"score\", ascending=False)\n",
    "score_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2289ef62-a0c9-4c8a-bbf3-f9cf0a012655",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "The Count Vectorizer converts cleaned text into a numerical format by counting word occurrences. It extracts the **top 10,000 most frequent words**, creating a matrix where each column represents a word and its frequency. The most common words are selected based on occurrence, making it useful for **text classification, topic modeling, and sentiment analysis** in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a536a7fd-078e-4d9a-9da3-9debcf622404",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample words in vocabulary:\", list(word2vec_cbow.wv.index_to_key)[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51411f0a-b5f1-4324-b35e-3c0692552740",
   "metadata": {},
   "source": [
    "#### Word2Vec CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "23d32db9-8868-4fb5-bb75-b010d4650e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = []\n",
    "for line in text_data:\n",
    "    clean_line = re.sub(r\"<.*?>\", \" \", line)  \n",
    "    clean_line = re.sub(r\"https?://\\S+\", \" \", clean_line)  \n",
    "    clean_line = re.sub(r\"[^a-zA-Z\\s]\", \" \", clean_line) \n",
    "    cleaned_text.append(clean_line.lower())  \n",
    "\n",
    "tokenized_sentences = [sentence.split() for sentence in cleaned_text if sentence]\n",
    "\n",
    "word2vec_cbow = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=2, workers=4, sg=0)\n",
    "\n",
    "word2vec_cbow.save(\"word2vec_cbow.model\")\n",
    "\n",
    "vocabulary_size = len(word2vec_cbow.wv)\n",
    "vocabulary_words = list(word2vec_cbow.wv.index_to_key)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "07584f5a-395e-470a-89ea-f1c61b9f19da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2284"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e2757d37-ce39-428e-a802-125d11d84a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'wikipedia',\n",
       " 'references',\n",
       " 'of',\n",
       " 'and',\n",
       " 'links',\n",
       " 'external',\n",
       " 'in',\n",
       " 'a',\n",
       " 'is']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "cd4789a8-2bce-4ef5-98f8-322110f0b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_to_check = \"wikipedia\"  # Choose a word from vocabulary\n",
    "if word_to_check in word2vec_cbow.wv:\n",
    "     similar_words = word2vec_cbow.wv.most_similar(word_to_check, topn=5)\n",
    "     similar_words\n",
    "else:\n",
    "     word_to_check\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e21f91ed-663f-4fb9-a992-96804ce5acb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 0.9993590116500854),\n",
       " ('the', 0.9993279576301575),\n",
       " ('a', 0.999306857585907),\n",
       " ('was', 0.999255359172821),\n",
       " ('and', 0.9992365837097168)]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1bc28bce-8727-4cfc-8984-6797089d8f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = {word: word2vec_cbow.wv[word] for word in word2vec_cbow.wv.index_to_key}\n",
    "df_word2vec = pd.DataFrame.from_dict(word_vectors, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4e378480-1d07-4f47-b9bd-8e7da49033f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.505877</td>\n",
       "      <td>0.374658</td>\n",
       "      <td>0.112767</td>\n",
       "      <td>0.489854</td>\n",
       "      <td>-0.061394</td>\n",
       "      <td>-1.002207</td>\n",
       "      <td>0.488773</td>\n",
       "      <td>1.199319</td>\n",
       "      <td>-0.506801</td>\n",
       "      <td>-0.381399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326439</td>\n",
       "      <td>-0.005702</td>\n",
       "      <td>0.250999</td>\n",
       "      <td>0.170419</td>\n",
       "      <td>0.904084</td>\n",
       "      <td>0.334856</td>\n",
       "      <td>-0.086891</td>\n",
       "      <td>-0.249460</td>\n",
       "      <td>0.066714</td>\n",
       "      <td>-0.098359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikipedia</th>\n",
       "      <td>-0.225289</td>\n",
       "      <td>0.165498</td>\n",
       "      <td>0.051778</td>\n",
       "      <td>0.211769</td>\n",
       "      <td>-0.015507</td>\n",
       "      <td>-0.434800</td>\n",
       "      <td>0.207854</td>\n",
       "      <td>0.519172</td>\n",
       "      <td>-0.218640</td>\n",
       "      <td>-0.169131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141831</td>\n",
       "      <td>-0.003860</td>\n",
       "      <td>0.106328</td>\n",
       "      <td>0.065268</td>\n",
       "      <td>0.381819</td>\n",
       "      <td>0.143996</td>\n",
       "      <td>-0.027592</td>\n",
       "      <td>-0.106726</td>\n",
       "      <td>0.017837</td>\n",
       "      <td>-0.039243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>references</th>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>-0.006743</td>\n",
       "      <td>-0.001256</td>\n",
       "      <td>0.007688</td>\n",
       "      <td>0.007161</td>\n",
       "      <td>-0.003605</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>-0.008432</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004450</td>\n",
       "      <td>0.005693</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>-0.004097</td>\n",
       "      <td>0.008153</td>\n",
       "      <td>0.005453</td>\n",
       "      <td>0.005871</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.008194</td>\n",
       "      <td>-0.007019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>-0.479540</td>\n",
       "      <td>0.357905</td>\n",
       "      <td>0.100746</td>\n",
       "      <td>0.447843</td>\n",
       "      <td>-0.042915</td>\n",
       "      <td>-0.933549</td>\n",
       "      <td>0.450844</td>\n",
       "      <td>1.118255</td>\n",
       "      <td>-0.459176</td>\n",
       "      <td>-0.360654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.295823</td>\n",
       "      <td>-0.008338</td>\n",
       "      <td>0.230432</td>\n",
       "      <td>0.156810</td>\n",
       "      <td>0.844464</td>\n",
       "      <td>0.306429</td>\n",
       "      <td>-0.064040</td>\n",
       "      <td>-0.235905</td>\n",
       "      <td>0.054583</td>\n",
       "      <td>-0.101981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>-0.384057</td>\n",
       "      <td>0.280466</td>\n",
       "      <td>0.071057</td>\n",
       "      <td>0.356747</td>\n",
       "      <td>-0.034922</td>\n",
       "      <td>-0.738154</td>\n",
       "      <td>0.361695</td>\n",
       "      <td>0.891870</td>\n",
       "      <td>-0.376830</td>\n",
       "      <td>-0.273822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247538</td>\n",
       "      <td>-0.010390</td>\n",
       "      <td>0.191689</td>\n",
       "      <td>0.124068</td>\n",
       "      <td>0.671807</td>\n",
       "      <td>0.237048</td>\n",
       "      <td>-0.050297</td>\n",
       "      <td>-0.173161</td>\n",
       "      <td>0.049270</td>\n",
       "      <td>-0.075054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1         2         3         4         5   \\\n",
       "the        -0.505877  0.374658  0.112767  0.489854 -0.061394 -1.002207   \n",
       "wikipedia  -0.225289  0.165498  0.051778  0.211769 -0.015507 -0.434800   \n",
       "references -0.000024  0.003191 -0.006743 -0.001256  0.007688  0.007161   \n",
       "of         -0.479540  0.357905  0.100746  0.447843 -0.042915 -0.933549   \n",
       "and        -0.384057  0.280466  0.071057  0.356747 -0.034922 -0.738154   \n",
       "\n",
       "                  6         7         8         9   ...        90        91  \\\n",
       "the         0.488773  1.199319 -0.506801 -0.381399  ...  0.326439 -0.005702   \n",
       "wikipedia   0.207854  0.519172 -0.218640 -0.169131  ...  0.141831 -0.003860   \n",
       "references -0.003605  0.002889 -0.008432  0.006089  ... -0.004450  0.005693   \n",
       "of          0.450844  1.118255 -0.459176 -0.360654  ...  0.295823 -0.008338   \n",
       "and         0.361695  0.891870 -0.376830 -0.273822  ...  0.247538 -0.010390   \n",
       "\n",
       "                  92        93        94        95        96        97  \\\n",
       "the         0.250999  0.170419  0.904084  0.334856 -0.086891 -0.249460   \n",
       "wikipedia   0.106328  0.065268  0.381819  0.143996 -0.027592 -0.106726   \n",
       "references  0.009195 -0.004097  0.008153  0.005453  0.005871  0.000409   \n",
       "of          0.230432  0.156810  0.844464  0.306429 -0.064040 -0.235905   \n",
       "and         0.191689  0.124068  0.671807  0.237048 -0.050297 -0.173161   \n",
       "\n",
       "                  98        99  \n",
       "the         0.066714 -0.098359  \n",
       "wikipedia   0.017837 -0.039243  \n",
       "references  0.008194 -0.007019  \n",
       "of          0.054583 -0.101981  \n",
       "and         0.049270 -0.075054  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word2vec.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1973d7-4e63-4d39-ab1d-536ede317cec",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "The **Word2Vec CBOW model** efficiently learns word representations by predicting target words from surrounding context, mapping words to numerical vectors while preserving semantic meaning. When trained on Wikipedia data, it captures relationships between words, making it useful for **synonym detection, sentiment analysis, topic modeling, and recommendation systems**. Frequent words like \"the\" and \"of\" may have similar vectors due to shared contexts, revealing linguistic patterns that enhance **document similarity analysis and other NLP applications**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd63e33-b298-471e-8bbf-a9a27c6edf0f",
   "metadata": {},
   "source": [
    "#### FastText Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "560e271e-7d56-4355-8c3b-ec6daad4679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = []\n",
    "for line in text_data:\n",
    "    clean_line = re.sub(r\"<.*?>\", \" \", line)  \n",
    "    clean_line = re.sub(r\"https?://\\S+\", \" \", clean_line)  \n",
    "    clean_line = re.sub(r\"[^a-zA-Z\\s]\", \" \", clean_line) \n",
    "    cleaned_text.append(clean_line.lower())  \n",
    "\n",
    "tokenized_sentences = [sentence.split() for sentence in cleaned_text if sentence]\n",
    "fasttext_model = FastText(sentences=tokenized_sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
    "fasttext_model.save(\"fasttext.model\")\n",
    "\n",
    "vocabulary_size = len(fasttext_model.wv)\n",
    "vocabulary_words = list(fasttext_model.wv.index_to_key)[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1c8df853-ba9d-4778-9a62-199a36ce412d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2284"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4c0fa9de-4e1e-4bbb-a7eb-11d42ccaff6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'wikipedia',\n",
       " 'references',\n",
       " 'of',\n",
       " 'and',\n",
       " 'links',\n",
       " 'external',\n",
       " 'in',\n",
       " 'a',\n",
       " 'is']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9b425d9c-5a79-426b-b576-3876f1f73cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_check = \"wikipedia\"\n",
    "if word_to_check in fasttext_model.wv:\n",
    "     similar_words = fasttext_model.wv.most_similar(word_to_check, topn=5)\n",
    "     similar_words\n",
    "else:\n",
    "     word_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "863998d1-303b-4970-b10c-d6ec3909c23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('international', 0.9999842047691345),\n",
       " ('national', 0.999984085559845),\n",
       " ('station', 0.9999839663505554),\n",
       " ('expedition', 0.9999829530715942),\n",
       " ('publication', 0.9999828934669495)]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "8e95b082-104e-480c-8626-298b8cf18f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.549158</td>\n",
       "      <td>1.143553</td>\n",
       "      <td>-0.596456</td>\n",
       "      <td>0.486956</td>\n",
       "      <td>0.060507</td>\n",
       "      <td>0.095754</td>\n",
       "      <td>1.004807</td>\n",
       "      <td>0.168418</td>\n",
       "      <td>0.727500</td>\n",
       "      <td>-1.229129</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.956318</td>\n",
       "      <td>-0.025237</td>\n",
       "      <td>0.109406</td>\n",
       "      <td>0.396868</td>\n",
       "      <td>-0.791044</td>\n",
       "      <td>0.891996</td>\n",
       "      <td>-0.192547</td>\n",
       "      <td>-0.655607</td>\n",
       "      <td>0.206663</td>\n",
       "      <td>0.428990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wikipedia</th>\n",
       "      <td>-0.204623</td>\n",
       "      <td>0.427920</td>\n",
       "      <td>-0.221578</td>\n",
       "      <td>0.180830</td>\n",
       "      <td>0.022302</td>\n",
       "      <td>0.035797</td>\n",
       "      <td>0.373685</td>\n",
       "      <td>0.061046</td>\n",
       "      <td>0.271519</td>\n",
       "      <td>-0.460898</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.359003</td>\n",
       "      <td>-0.007091</td>\n",
       "      <td>0.040103</td>\n",
       "      <td>0.148960</td>\n",
       "      <td>-0.295369</td>\n",
       "      <td>0.333775</td>\n",
       "      <td>-0.073865</td>\n",
       "      <td>-0.242573</td>\n",
       "      <td>0.077421</td>\n",
       "      <td>0.160182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>references</th>\n",
       "      <td>-0.130237</td>\n",
       "      <td>0.268477</td>\n",
       "      <td>-0.140577</td>\n",
       "      <td>0.115670</td>\n",
       "      <td>0.015091</td>\n",
       "      <td>0.023505</td>\n",
       "      <td>0.237676</td>\n",
       "      <td>0.041854</td>\n",
       "      <td>0.170432</td>\n",
       "      <td>-0.290017</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227372</td>\n",
       "      <td>-0.004180</td>\n",
       "      <td>0.025941</td>\n",
       "      <td>0.094831</td>\n",
       "      <td>-0.187895</td>\n",
       "      <td>0.212854</td>\n",
       "      <td>-0.044905</td>\n",
       "      <td>-0.154460</td>\n",
       "      <td>0.048603</td>\n",
       "      <td>0.101314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>-0.429241</td>\n",
       "      <td>0.898569</td>\n",
       "      <td>-0.468397</td>\n",
       "      <td>0.378014</td>\n",
       "      <td>0.055372</td>\n",
       "      <td>0.077033</td>\n",
       "      <td>0.782779</td>\n",
       "      <td>0.133990</td>\n",
       "      <td>0.574239</td>\n",
       "      <td>-0.962228</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.752277</td>\n",
       "      <td>-0.016287</td>\n",
       "      <td>0.088254</td>\n",
       "      <td>0.311832</td>\n",
       "      <td>-0.617090</td>\n",
       "      <td>0.700275</td>\n",
       "      <td>-0.151610</td>\n",
       "      <td>-0.513414</td>\n",
       "      <td>0.160595</td>\n",
       "      <td>0.332671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>-0.458993</td>\n",
       "      <td>0.949453</td>\n",
       "      <td>-0.490593</td>\n",
       "      <td>0.405358</td>\n",
       "      <td>0.050956</td>\n",
       "      <td>0.084037</td>\n",
       "      <td>0.829974</td>\n",
       "      <td>0.135896</td>\n",
       "      <td>0.603638</td>\n",
       "      <td>-1.012258</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.792857</td>\n",
       "      <td>-0.021676</td>\n",
       "      <td>0.094440</td>\n",
       "      <td>0.332525</td>\n",
       "      <td>-0.651415</td>\n",
       "      <td>0.742039</td>\n",
       "      <td>-0.161193</td>\n",
       "      <td>-0.537828</td>\n",
       "      <td>0.171364</td>\n",
       "      <td>0.356782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1         2         3         4         5   \\\n",
       "the        -0.549158  1.143553 -0.596456  0.486956  0.060507  0.095754   \n",
       "wikipedia  -0.204623  0.427920 -0.221578  0.180830  0.022302  0.035797   \n",
       "references -0.130237  0.268477 -0.140577  0.115670  0.015091  0.023505   \n",
       "of         -0.429241  0.898569 -0.468397  0.378014  0.055372  0.077033   \n",
       "and        -0.458993  0.949453 -0.490593  0.405358  0.050956  0.084037   \n",
       "\n",
       "                  6         7         8         9   ...        90        91  \\\n",
       "the         1.004807  0.168418  0.727500 -1.229129  ... -0.956318 -0.025237   \n",
       "wikipedia   0.373685  0.061046  0.271519 -0.460898  ... -0.359003 -0.007091   \n",
       "references  0.237676  0.041854  0.170432 -0.290017  ... -0.227372 -0.004180   \n",
       "of          0.782779  0.133990  0.574239 -0.962228  ... -0.752277 -0.016287   \n",
       "and         0.829974  0.135896  0.603638 -1.012258  ... -0.792857 -0.021676   \n",
       "\n",
       "                  92        93        94        95        96        97  \\\n",
       "the         0.109406  0.396868 -0.791044  0.891996 -0.192547 -0.655607   \n",
       "wikipedia   0.040103  0.148960 -0.295369  0.333775 -0.073865 -0.242573   \n",
       "references  0.025941  0.094831 -0.187895  0.212854 -0.044905 -0.154460   \n",
       "of          0.088254  0.311832 -0.617090  0.700275 -0.151610 -0.513414   \n",
       "and         0.094440  0.332525 -0.651415  0.742039 -0.161193 -0.537828   \n",
       "\n",
       "                  98        99  \n",
       "the         0.206663  0.428990  \n",
       "wikipedia   0.077421  0.160182  \n",
       "references  0.048603  0.101314  \n",
       "of          0.160595  0.332671  \n",
       "and         0.171364  0.356782  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = {word: fasttext_model.wv[word] for word in fasttext_model.wv.index_to_key}\n",
    "df_fasttext = pd.DataFrame.from_dict(word_vectors, orient='index')\n",
    "df_fasttext.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25282d4-101d-4010-9e3f-229bb8d6d2a0",
   "metadata": {},
   "source": [
    "#### observation\n",
    "FastText is trained on tokenized sentences from Wikipedia data, creating dense word embeddings with **100-dimensional vectors**. The model considers **subword units (character n-grams)**, allowing it to handle out-of-vocabulary (OOV) words better than Word2Vec. The trained model is saved as `\"fasttext.model\"`, and key outputs include the **vocabulary size**, **sample words**, and **most similar words** to `\"wikipedia\"`. Since FastText captures morphological similarities, it helps in NLP tasks like **text classification, information retrieval, and named entity recognition (NER)** by generating meaningful word representations even for unseen words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312bc426-375e-4039-b734-adb7e9f9184c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29048a3-d0c3-4ae0-9e22-9f27f10f5374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88486743-dc40-41e4-ae39-49954ffe6b03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
